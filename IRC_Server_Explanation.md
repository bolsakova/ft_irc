# Пошаговое объяснение IRC сервера (текст на 15 минут)

## 1. Запуск программы: main.cpp

Начнем с самого начала — с функции **main**. Когда мы запускаем наш IRC сервер, программа ожидает три аргумента командной строки:
- Имя программы (автоматически)
- Порт (например, 6667)
- Пароль для подключения к серверу

**Что происходит в main:**

Первым делом main проверяет, что передано ровно 3 аргумента. Если нет — выводит сообщение об использовании и завершается.

Затем проверяется корректность порта: он должен быть в диапазоне от 1024 до 65535. Почему именно этот диапазон? Порты от 0 до 1023 зарезервированы системой и требуют root-прав. Мы используем непривилегированные порты.

Проверяется, что пароль не пустой.

После всех проверок создается объект **Server**, передавая ему порт и пароль. Важный момент: мы сохраняем указатель на этот сервер в глобальную переменную `g_server`. Зачем? Чтобы обработчик сигналов мог корректно остановить сервер при нажатии Ctrl+C.

Далее устанавливаются обработчики сигналов SIGINT (Ctrl+C) и SIGTERM (команда kill). Когда приходит сигнал, вызывается функция `signalHandler`, которая через глобальный указатель вызывает `server.stop()` для корректного завершения.

Наконец, вызывается `server.run()` — это бесконечный цикл, который будет обрабатывать все подключения клиентов. Программа работает до тех пор, пока не получит сигнал остановки.

Весь код обернут в try-catch блок, чтобы перехватывать любые исключения и корректно завершать программу с выводом ошибки.

---

## 2. Создание сервера: конструктор Server

Когда создается объект `Server`, происходит инициализация всех компонентов сервера.

**Что делает конструктор:**

Сначала вызывается функция `ignore_sigpipe()`. Это критически важно! В сетевых приложениях, когда мы пытаемся отправить данные клиенту, который уже отключился, система посылает сигнал SIGPIPE, который по умолчанию убивает программу. Мы блокируем этот сигнал, чтобы сервер не падал при каждом отключении клиента.

Затем вызывается `initSocket(port)` — это функция создания и настройки слушающего сокета. Разберем ее подробно:

**initSocket шаг за шагом:**

1. **Валидация порта:** проверяем, что порт находится в допустимом диапазоне
2. **Создание сокета:** вызываем `socket(AF_INET, SOCK_STREAM, 0)`. AF_INET означает IPv4, SOCK_STREAM — это TCP соединение (надежная передача с установкой соединения)
3. **Установка SO_REUSEADDR:** это позволяет быстро перезапустить сервер после аварийного завершения. Без этого флага ОС может держать порт занятым несколько минут
4. **Bind (привязка):** связываем сокет с конкретным портом на всех сетевых интерфейсах (0.0.0.0). Создаем структуру `sockaddr_in`, заполняем ее нулями, устанавливаем семейство адресов IPv4, адрес INADDR_ANY (слушаем на всех интерфейсах) и порт в сетевом порядке байтов (big-endian)
5. **Listen (прослушивание):** переводим сокет в режим прослушивания с очередью SOMAXCONN (максимальное количество ожидающих подключений, которое поддерживает система)
6. **Non-blocking режим:** устанавливаем сокет в неблокирующий режим с помощью fcntl. Это ключевой момент! В неблокирующем режиме функции recv/send/accept не будут "зависать" в ожидании — они либо сразу выполняют операцию, либо возвращают EAGAIN/EWOULDBLOCK

Если на любом из этих шагов происходит ошибка, мы закрываем сокет и выбрасываем исключение.

После успешной инициализации сокета конструктор создает **CommandHandler** — объект, который будет разбирать и обрабатывать IRC команды от клиентов.

Наконец, добавляем слушающий сокет в `m_poll_fds` — вектор дескрипторов для функции poll(). Устанавливаем событие POLLIN, что означает "уведоми меня, когда появятся новые подключения".

Весь процесс инициализации обернут в try-catch: если что-то пойдет не так, мы закрываем сокет и пробрасываем исключение дальше в main.

---

## 3. Структуры данных сервера

Сервер хранит свое состояние в трех ключевых структурах данных:

### 1. `std::vector<pollfd> m_poll_fds`

Это динамический массив структур pollfd. Каждая структура содержит:
- `fd` — дескриптор файла (сокета)
- `events` — какие события нас интересуют (POLLIN для чтения, POLLOUT для записи)
- `revents` — какие события произошли (заполняется функцией poll)

Почему вектор? Потому что количество подключенных клиентов меняется динамически — клиенты подключаются и отключаются. Вектор позволяет легко добавлять и удалять элементы.

### 2. `std::map<int, std::unique_ptr<Client>> m_clients`

Это словарь (ассоциативный массив), где ключ — дескриптор сокета (int), а значение — умный указатель на объект Client.

Почему map? Потому что нам нужен быстрый поиск клиента по дескриптору. Когда poll() сообщает "на дескрипторе 5 есть данные", мы мгновенно находим соответствующего клиента за O(log n).

Почему unique_ptr? Это RAII идиома — автоматическое управление памятью. Когда мы удаляем клиента из map (вызываем erase), unique_ptr автоматически освобождает память. Не нужно вручную вызывать delete, не будет утечек памяти, код защищен от исключений.

### 3. `std::map<std::string, std::unique_ptr<Channel>> m_channels`

Словарь, где ключ — имя канала (строка типа "#general"), а значение — умный указатель на объект Channel.

Почему map? Быстрый поиск канала по имени. Когда клиент отправляет "JOIN #general", мы моментально находим или создаем этот канал.

Почему unique_ptr? Та же причина — автоматическая очистка памяти при удалении канала.

---

## 4. Главный цикл: Server::run()

После инициализации вызывается `server.run()` — это сердце нашего сервера. Здесь происходит вся магия обработки множества клиентов одновременно.

### Архитектура: однопоточный неблокирующий I/O с poll()

Важно понять: наш сервер работает в **одном потоке**. Нет никаких thread, никакой многопоточности. Как же мы обрабатываем много клиентов одновременно?

Секрет в функции **poll()**. Это системный вызов, который принимает массив дескрипторов и говорит: "Разбуди меня, когда хотя бы на одном из этих дескрипторов произойдет событие".

### Главный цикл выглядит так:

```cpp
while (m_running) {
    // Один вызов poll() для ВСЕХ сокетов
    poll(&m_poll_fds[0], m_poll_fds.size(), -1);
    
    // Проверяем каждый дескриптор
    for каждого дескриптора:
        if это слушающий сокет:
            acceptClient() — принимаем новое подключение
        else если это клиентский сокет:
            if POLLIN (данные для чтения):
                receiveData() — читаем данные от клиента
            if POLLOUT (готов к записи):
                sendData() — отправляем данные клиенту
            if POLLERR/POLLHUP (ошибка/отключение):
                disconnectClient() — закрываем соединение
}
```

### Ключевые моменты:

- **Один вызов poll()** мониторит ВСЕ сокеты — и слушающий, и все клиентские
- poll() **блокирует** (спит) до тех пор, пока не произойдет событие хотя бы на одном сокете
- Когда событие происходит, poll() возвращает управление, и мы проверяем, на каких именно дескрипторах что-то случилось
- Обрабатываем события по очереди, затем снова вызываем poll()

Это называется **event loop** (цикл событий) — классический паттерн для высокопроизводительных сетевых серверов.

---

## 5. Новое подключение: acceptClient()

Когда на слушающем сокете происходит событие POLLIN, это означает, что есть новое входящее подключение.

### Что делает acceptClient:

Вызывается `accept()` — системный вызов, который принимает новое подключение и создает новый сокет для общения с этим клиентом. Важно: слушающий сокет остается нетронутым, он продолжает принимать новые подключения.

accept() возвращает новый дескриптор `client_fd`. Если возвращается -1, проверяем errno:
- EAGAIN/EWOULDBLOCK — это нормально, означает "больше нет ожидающих подключений"
- Любая другая ошибка — логируем и продолжаем

Для нового клиента:
1. **Устанавливаем неблокирующий режим** для клиентского сокета
2. **Добавляем в m_poll_fds** — теперь poll() будет следить за этим сокетом. Устанавливаем событие POLLIN (хотим читать данные от клиента)
3. **Создаем объект Client** и добавляем в m_clients с ключом client_fd

Теперь этот клиент полностью интегрирован в систему, и в следующей итерации цикла poll() будет мониторить его сокет.

---

## 6. Чтение данных: receiveData()

Когда poll() сообщает POLLIN на клиентском сокете, вызывается `receiveData(fd)`.

### Что происходит:

Вызываем `recv(fd, buffer, size, 0)` в цикле, пока:
- recv() возвращает > 0 (получены данные) — добавляем в входной буфер клиента
- recv() возвращает 0 (клиент закрыл соединение) — помечаем клиента как "peer_closed"
- recv() возвращает -1 с EAGAIN/EWOULDBLOCK — больше нет данных, выходим из цикла
- recv() возвращает -1 с другой ошибкой — отключаем клиента

### Обработка полученных данных:

Данные добавляются в `m_inbuf` (входной буфер) клиента. Почему буфер? Потому что TCP — это поток байтов, не сообщения. Мы можем получить:
- Половину команды: "NI" (остальное "CK alice\r\n" придет позже)
- Несколько команд сразу: "NICK alice\r\nUSER alice 0 * :Real\r\n"

Поэтому мы ищем полные команды (заканчивающиеся на "\r\n") и обрабатываем их:

```cpp
while (клиент имеет полную команду):
    извлекаем команду из буфера
    передаем в CommandHandler для обработки
    если появились данные для отправки:
        включаем POLLOUT для этого дескриптора
```

---

## 7. Обработка команд: CommandHandler

**CommandHandler** — это мозг сервера, который понимает IRC протокол.

Когда receiveData извлекает полную команду, она передается в `handleCommand(raw_command, client)`.

### Что делает handleCommand:

1. **Парсинг:** используя класс Parser, разбирает текстовую команду на составные части:
   - Команда (NICK, JOIN, PRIVMSG и т.д.)
   - Параметры
   
   Например: "NICK alice\r\n" → команда="NICK", параметры=["alice"]

2. **Маршрутизация:** в зависимости от команды вызывает соответствующий обработчик:
   - PASS → handlePass()
   - NICK → handleNick()
   - USER → handleUser()
   - JOIN → handleJoin()
   - PRIVMSG → handlePrivmsg()
   - и так далее

### Типичный обработчик команды:

Рассмотрим handleNick (установка никнейма):
1. Проверяет, аутентифицирован ли клиент (PASS уже отправлен)
2. Валидирует никнейм (длина, допустимые символы)
3. Проверяет, не занят ли этот никнейм другим клиентом
4. Если все ОК — устанавливает новый никнейм
5. Если это первая регистрация (NICK + USER) — отправляет приветственные сообщения
6. Если никнейм уже используется — отправляет ошибку ERR_NICKNAMEINUSE

### Отправка ответов:

Все ответы добавляются в `m_outbuf` (выходной буфер) клиента. Не отправляются сразу! Почему? Потому что в неблокирующем режиме send() может не отправить все данные сразу, если сетевой буфер заполнен.

После добавления данных в outbuf, мы включаем событие POLLOUT для этого дескриптора, чтобы poll() уведомил нас, когда сокет готов к записи.

---

## 8. Отправка данных: sendData()

Когда poll() сообщает POLLOUT на клиентском сокете, вызывается `sendData(fd)`.

### Что происходит:

1. Проверяем, есть ли данные в `m_outbuf` клиента
2. Если буфер пустой — отключаем POLLOUT и выходим
3. Если есть данные — вызываем `send(fd, data, size, MSG_NOSIGNAL)`
4. send() может отправить:
   - Все данные — супер, очищаем буфер
   - Часть данных — нормально, удаляем отправленную часть из буфера
   - Ничего (EAGAIN) — ОК, попробуем в следующий раз
   - Ошибка — отключаем клиента

5. Если буфер опустел — отключаем POLLOUT (не нужно больше мониторить запись)
6. Если клиент закрыл соединение (peer_closed) И буфер пустой — отключаем клиента полностью

**Важный момент:** мы используем флаг MSG_NOSIGNAL при send(), чтобы не получить SIGPIPE, если клиент уже отключился.

---

## 9. Управление состоянием клиента: класс Client

Каждый подключенный клиент представлен объектом **Client**, который хранит:

### Сетевое состояние:
- `m_fd` — дескриптор сокета
- `m_inbuf` — входной буфер (неполные команды)
- `m_outbuf` — выходной буфер (данные для отправки)
- `m_peer_closed` — клиент закрыл свою сторону соединения
- `m_should_disconnect` — флаг для отложенного отключения

### IRC состояние:
- `m_nickname` — никнейм пользователя
- `m_username` — имя пользователя
- `m_realname` — реальное имя
- `m_authenticated` — прошел ли проверку пароля (PASS)
- `m_registered` — зарегистрирован ли (NICK + USER)

### Зачем два флага authenticated и registered?

IRC протокол требует:
1. Сначала PASS (аутентификация)
2. Затем NICK (установка никнейма)
3. Затем USER (установка username)

Только после всех трех шагов клиент считается полностью зарегистрированным и может пользоваться сервером.

---

## 10. Управление каналами: класс Channel

**Channel** представляет IRC канал (например, #general) и хранит:

### Участники:
- `m_members` — список обычных участников
- `m_operators` — список операторов канала (имеют права управления)

### Настройки канала:
- `m_topic` — тема канала
- `m_key` — пароль канала (если установлен)
- `m_invited_users` — список приглашенных пользователей (для режима +i)

### Режимы (modes):
- `+i` (invite-only) — только по приглашению
- `+t` (topic-restricted) — только операторы могут менять тему
- `+k` (key) — требуется пароль
- `+l` (limit) — лимит участников

### Основные операции:

- **JOIN:** клиент присоединяется к каналу. Проверяется пароль (если +k), наличие в списке приглашенных (если +i), лимит участников (если +l)
- **PART:** клиент покидает канал
- **PRIVMSG:** отправка сообщения всем участникам канала (broadcast)
- **MODE:** изменение режимов канала (только операторы)
- **TOPIC:** установка/просмотр темы канала
- **KICK:** исключение пользователя из канала (только операторы)
- **INVITE:** приглашение пользователя в канал (только операторы, если +i)

---

## 11. Отключение клиента: disconnectClient()

Когда клиент отключается (сам или по ошибке), вызывается `disconnectClient(fd)`:

1. **Удаляем из m_poll_fds** — poll() больше не следит за этим дескриптором
2. **Закрываем сокет** — вызываем close(fd), освобождаем системный ресурс
3. **Удаляем из m_clients** — вызываем m_clients.erase(fd)

Важный момент: когда мы вызываем erase(), unique_ptr автоматически удаляет объект Client. Память освобождается, деструктор Client вызывается автоматически. Никаких ручных delete, никаких утечек!

---

## 12. Корректное завершение: stop()

Когда приходит сигнал SIGINT (Ctrl+C), вызывается `server.stop()`:

1. **Отправка уведомления:** всем клиентам отправляется сообщение "Server shutting down"
2. **Попытка отправить** данные из буферов (вызываем sendData для каждого клиента)
3. **Отключение всех клиентов** — вызываем disconnectClient для каждого
4. **Закрытие слушающего сокета**
5. **Очистка векторов и map**

Сервер корректно завершает работу, не оставляя висящих соединений или незакрытых файловых дескрипторов.

---

## Заключение

Наш IRC сервер — это классический пример **однопоточного неблокирующего сервера с event loop**:

- **Один поток** обрабатывает все соединения
- **poll()** эффективно мониторит множество сокетов одновременно
- **Неблокирующий I/O** гарантирует, что сервер никогда не "зависает"
- **unique_ptr** обеспечивает автоматическое управление памятью
- **RAII идиома** делает код exception-safe

Это производительная, масштабируемая архитектура, используемая в реальных серверах (nginx, Node.js, Redis используют похожие подходы).

---

## Ключевые концепции для защиты

### 1. Почему poll(), а не threads?
- Один системный вызов вместо множества переключений контекста
- Нет race conditions и синхронизации
- Проще отлаживать
- Меньше потребление памяти (каждый thread требует свой стек ~1MB)

### 2. Почему неблокирующий I/O?
- Позволяет одному потоку обрабатывать тысячи соединений
- Операции не зависают в ожидании
- Идеально подходит для event-driven архитектуры

### 3. Почему unique_ptr?
- Автоматическое освобождение памяти (RAII)
- Исключает утечки памяти
- Exception-safe код
- Нулевые накладные расходы (zero overhead)
- Явное владение ресурсами

### 4. Почему map для clients/channels?
- O(log n) поиск по ключу
- Автоматическая сортировка
- Стабильные итераторы при вставке/удалении других элементов

### 5. Почему vector для poll_fds?
- Последовательный доступ к памяти (cache-friendly)
- poll() требует непрерывный массив
- Динамический размер для переменного количества клиентов
